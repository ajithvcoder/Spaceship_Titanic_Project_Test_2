{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ajith/projects/Spaceship_Titanic_MLOps_Project/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ajith/projects/Spaceship_Titanic_MLOps_Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    local_data_file: Path\n",
    "    cloud_config_zipfile: Path\n",
    "    authentication_token: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from titanicSpaceShip.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import urllib.request as request\n",
    "import zipfile\n",
    "from titanicSpaceShip import logger\n",
    "from titanicSpaceShip.utils.common import get_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "import json\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def download_file(self):\n",
    "        delay = 5\n",
    "        max_retries = 1\n",
    "        for _ in range(max_retries):\n",
    "            try:\n",
    "                if not os.path.exists(self.config.local_data_file):\n",
    "                    # This secure connect bundle is autogenerated when you download your SCB, \n",
    "                    # if yours is different update the file name below\n",
    "                    cloud_config= {\n",
    "                    'secure_connect_bundle': self.config.cloud_config_zipfile\n",
    "                    }\n",
    "                    # This token JSON file is autogenerated when you download your token, \n",
    "                    # if yours is different update the file name below\n",
    "                    with open(self.config.authentication_token) as f:\n",
    "                        secrets = json.load(f)\n",
    "                    CLIENT_ID = secrets[\"clientId\"]\n",
    "                    CLIENT_SECRET = secrets[\"secret\"]\n",
    "\n",
    "                    auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)\n",
    "                    cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "                    session = cluster.connect()\n",
    "\n",
    "                    row = session.execute('SELECT * FROM \"prediction\".\"data\"')\n",
    "                    if row:\n",
    "                        # Specify the CSV file path\n",
    "                        csv_file_path = self.config.local_data_file\n",
    "\n",
    "                        # Write named tuples to CSV file\n",
    "                        with open(csv_file_path, 'w', newline='') as csv_file:\n",
    "                            writer = csv.writer(csv_file)\n",
    "                            \n",
    "                            # Write header\n",
    "                            writer.writerow(row[0]._fields)\n",
    "                            \n",
    "                            # Write data\n",
    "                            for each in row:\n",
    "                                writer.writerow(each)\n",
    "                            logger.info(f\"{self.config.local_data_file} download! from astra db with following info: \\n {row[0]._fields}\")\n",
    "                    else:\n",
    "                        logger.info(f\"An Error occurred while connecting to db\")\n",
    "                    \n",
    "                else:\n",
    "                    logger.info(f\"File already exists of size: {get_size(Path(self.config.local_data_file))}\")\n",
    "            except Exception as e:\n",
    "                logger.info(f\"Delay for next attempt {e}\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "        else:\n",
    "            logger.info(f\"Failed connecting to astra/casandra db after {max_retries} attempt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from titanicSpaceShip.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            self.params = read_yaml(params_filepath)\n",
    "\n",
    "            create_directories([self.config.artifacts_root])\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "          config = self.config.data_ingestion\n",
    "\n",
    "          create_directories([config.root_dir])\n",
    "\n",
    "          data_ingestion_config = DataIngestionConfig(\n",
    "                root_dir=config.root_dir,\n",
    "                local_data_file=config.local_data_file,\n",
    "                cloud_config_zipfile=config.cloud_config_zipfile,\n",
    "                authentication_token=config.authentication_token\n",
    "          )\n",
    "          return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-14 12:01:12,046: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-01-14 12:01:12,052: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-01-14 12:01:12,058: INFO: common: created directory at: artifacts]\n",
      "[2024-01-14 12:01:12,060: INFO: common: created directory at: artifacts/data_ingestion]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-14 12:01:12,656: WARNING: 681926982: Downgrading core protocol version from 66 to 65 for 43c9e6f8-a643-4970-8ad4-6fe67303cbd1-asia-south1.db.astra.datastax.com:29042:056a4a91-2687-448b-9873-6730a3416eea. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version]\n",
      "[2024-01-14 12:01:12,788: WARNING: 681926982: Downgrading core protocol version from 65 to 5 for 43c9e6f8-a643-4970-8ad4-6fe67303cbd1-asia-south1.db.astra.datastax.com:29042:056a4a91-2687-448b-9873-6730a3416eea. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version]\n",
      "[2024-01-14 12:01:12,960: ERROR: asyncorereactor: Closing connection <AsyncoreConnection(140285896216080) 43c9e6f8-a643-4970-8ad4-6fe67303cbd1-asia-south1.db.astra.datastax.com:29042:056a4a91-2687-448b-9873-6730a3416eea> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"]\n",
      "[2024-01-14 12:01:12,962: WARNING: 681926982: Downgrading core protocol version from 5 to 4 for 43c9e6f8-a643-4970-8ad4-6fe67303cbd1-asia-south1.db.astra.datastax.com:29042:056a4a91-2687-448b-9873-6730a3416eea. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version]\n",
      "[2024-01-14 12:01:13,280: INFO: policies: Using datacenter 'asia-south1' for DCAwareRoundRobinPolicy (via host '43c9e6f8-a643-4970-8ad4-6fe67303cbd1-asia-south1.db.astra.datastax.com:29042:056a4a91-2687-448b-9873-6730a3416eea'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes]\n",
      "[2024-01-14 12:01:14,220: WARNING: 681926982: Using index operator on paged results causes entire result set to be materialized.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69861/681926982.py:43: DeprecationWarning: ResultSet indexing support will be removed in 4.0. Consider using ResultSet.one() to get a single row.\n",
      "  writer.writerow(row[0]._fields)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-14 12:01:14,543: INFO: 681926982: artifacts/data_ingestion/data_spaceShip.csv download! from astra db with following info: \n",
      " ('SNo', 'Age', 'CryoSleep', 'Destination', 'FoodCourt', 'HomePlanet', 'RoomService', 'ShoppingMall', 'Spa', 'Transported', 'VIP', 'VRDeck')]\n",
      "[2024-01-14 12:01:14,544: INFO: 681926982: Failed connecting to astra/casandra db after 1 attempt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_69861/681926982.py:48: DeprecationWarning: ResultSet indexing support will be removed in 4.0. Consider using ResultSet.one() to get a single row.\n",
      "  logger.info(f\"{self.config.local_data_file} download! from astra db with following info: \\n {row[0]._fields}\")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.download_file()\n",
    "    # data_ingestion.extract_zip_file()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaceship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
